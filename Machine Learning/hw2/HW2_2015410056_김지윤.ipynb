{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW2\n",
    "\n",
    "#### Machine Learning in Korea University\n",
    "#### COSE362, Fall 2018\n",
    "#### Due : 11/26 (TUE) 11:59 PM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this assignment, you will learn various classification methods with given datasets.\n",
    "* Implementation detail: Anaconda 5.3 with python 3.7\n",
    "* Use given dataset. Please do not change train / valid / test split.\n",
    "* Use numpy, scikit-learn, and matplotlib library\n",
    "* You don't have to use all imported packages below. (some are optional). <br>\n",
    "Also, you can import additional packages in \"(Option) Other Classifiers\" part. \n",
    "* <b>*DO NOT MODIFY OTHER PARTS OF CODES EXCEPT \"Your Code Here\"*</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic packages\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Machine Learning Models\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Additional packages\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import your own packages if you need(only in scikit-learn, numpy, pandas).\n",
    "# Your Code Here\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.metrics import make_scorer\n",
    "#End Your Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process\n",
    "\n",
    "> 1. Load \"train.csv\". It includes all samples' features and labels.\n",
    "> 2. Training four types of classifiers(logistic regression, decision tree, random forest, support vector machine) and <b>validate</b> it in your own way. <b>(You can't get full credit if you don't conduct validation)</b>\n",
    "> 3. Optionally, if you would train your own classifier(e.g. ensembling or gradient boosting), you can evaluate your own model on the development data. <br>\n",
    "> 4. <b>You should submit your predicted results on test data with the selected classifier in your own manner.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task & dataset description\n",
    "1. 6 Features (1~6)<br>\n",
    "Feature 2, 4, 6 : Real-valued<br>\n",
    "Feature 1, 3, 5 : Categorical <br>\n",
    "\n",
    "2. Samples <br>\n",
    ">In development set : 2,000 samples <br>\n",
    ">In test set : 1,500 samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load development dataset\n",
    "Load your development dataset. You should read <b>\"train.csv\"</b>. This is a classification task, and you need to preprocess your data for training your model. <br>\n",
    "> You need to use <b>1-of-K coding scheme</b>, to convert categorical features to one-hot vector. <br>\n",
    "> For example, if there are 3 categorical values, you can convert these features as [1,0,0], [0,1,0], [0,0,1] by 1-of-K coding scheme. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class frequency\n",
      "==================================================\n",
      "[[  0 192]\n",
      " [  1 105]\n",
      " [  2 219]\n",
      " [  3 162]\n",
      " [  4  30]\n",
      " [  5  17]\n",
      " [  6 337]\n",
      " [  7 124]\n",
      " [  8   6]\n",
      " [  9  36]\n",
      " [ 10  48]\n",
      " [ 11  31]\n",
      " [ 12 119]\n",
      " [ 13 300]\n",
      " [ 14   4]\n",
      " [ 15 248]\n",
      " [ 16  19]\n",
      " [ 17   3]]\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2885, 23)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For training your model, you need to convert categorical features to one-hot encoding vectors.\n",
    "# Your Code Here\n",
    "df = pd.read_csv('./data/train.csv')\n",
    "df_X = df.iloc[:, 0:6].as_matrix()\n",
    "df_Y = df.iloc[:, 6:7]\n",
    "\n",
    "y = df_Y.values\n",
    "y = y.reshape(-1)\n",
    "\n",
    "print(\"class frequency\")\n",
    "print('='*50)\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "print (np.asarray((unique, counts)).T)\n",
    "print('='*50)\n",
    "            \n",
    "# One-hot encoding\n",
    "categorical_features = [0, 2, 4]\n",
    "feature_dict = {'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5, 'g': 6, 'h': 7}\n",
    "\n",
    "for feature_num in categorical_features:\n",
    "    feature_values_tr = df_X[:, feature_num]\n",
    "    feature_set = set(feature_values_tr)\n",
    "    \n",
    "    for i, value in enumerate(feature_values_tr):\n",
    "        feature_values_tr[i] = feature_dict[value]\n",
    "        \n",
    "    one_hot_matrix_tr = np.eye(len(feature_set))[feature_values_tr.astype(int)]\n",
    "    df_X = np.concatenate((df_X, one_hot_matrix_tr), axis=1)\n",
    "\n",
    "df_X = np.delete(df_X, categorical_features, 1)\n",
    "X = df_X.astype(int)\n",
    "\n",
    "# Oversampling\n",
    "up_sample_idx = [4, 5, 8, 9, 10, 11, 14, 16, 17]\n",
    "for idx in range(len(X)) :\n",
    "    if y[idx] in up_sample_idx :\n",
    "        for i in range(int(np.round(100/counts[y[idx]]))) :\n",
    "            X = np.vstack([X, X[idx]])\n",
    "            y = np.append(y, y[idx])\n",
    "\n",
    "X.shape\n",
    "# End Your Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "Train and validate your <b>logistic regression classifier</b>, and print out your validation(or cross-validation) error.\n",
    "> If you want, you can use cross validation, regularization, or feature selection methods. <br>\n",
    "> <b> You should use F1 score('macro' option) as evaluation metric. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "coef value:  0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.011613110035650399\n",
      "0.011688620301740093\n",
      "==================================================\n",
      "coef value:  0.001\n",
      "0.06929917160805454\n",
      "0.05847472223665208\n",
      "==================================================\n",
      "coef value:  0.01\n",
      "0.26326482815135954\n",
      "0.22817332762421452\n",
      "==================================================\n",
      "coef value:  0.1\n",
      "0.40660603602463796\n",
      "0.3651082709319086\n",
      "==================================================\n",
      "coef value:  1.0\n",
      "0.4395733182806293\n",
      "0.402887158032201\n",
      "==================================================\n",
      "coef value:  10.0\n",
      "0.4501796083928432\n",
      "0.4100008942879792\n",
      "==================================================\n",
      "coef value:  100.0\n",
      "0.4529534685532154\n",
      "0.413422528623253\n",
      "==================================================\n",
      "coef value:  1000.0\n",
      "0.4532004499041137\n",
      "0.41420250402680325\n",
      "==================================================\n",
      "max_tr_score 0.4532004499041137\n",
      "max_va_score 0.41420250402680325\n",
      "optimum coef 1000.0\n"
     ]
    }
   ],
   "source": [
    "# Training your logistic regression classifier, and print out your validation(or cross-validation) error.\n",
    "# Save your own model\n",
    "# Your Code Here\n",
    "valid_split = 1/10\n",
    "cv = ShuffleSplit(n_splits=10, test_size=valid_split, random_state=0)\n",
    "\n",
    "coefs = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]\n",
    "\n",
    "max_tr_score = 0\n",
    "max_va_score = 0\n",
    "optimum_coef = -1\n",
    "\n",
    "for coef in coefs:\n",
    "    print('='*50)\n",
    "    print(\"coef value: \", coef)\n",
    "    model = LogisticRegression(C=coef, solver='lbfgs', max_iter=500)\n",
    "\n",
    "    train_score = []\n",
    "    valid_score = []\n",
    "\n",
    "    for train_index, test_index in cv.split(X) :\n",
    "        X_tr, X_va = X[train_index], X[test_index]\n",
    "        y_tr, y_va = y[train_index], y[test_index]\n",
    "    \n",
    "        model.fit(X_tr, y_tr)\n",
    "        train_score.append(f1_score(y_tr, model.predict(X_tr), average='macro'))\n",
    "        valid_score.append(f1_score(y_va, model.predict(X_va), average='macro'))\n",
    "        \n",
    "    print(np.mean(train_score))\n",
    "    print(np.mean(valid_score))\n",
    "    \n",
    "    if np.mean(valid_score) > max_va_score :\n",
    "        max_tr_score = np.mean(train_score)\n",
    "        max_va_score = np.mean(valid_score)\n",
    "        optimum_coef = coef\n",
    "\n",
    "print('='*50)\n",
    "print(\"max_tr_score\", (max_tr_score))\n",
    "print(\"max_va_score\", (max_va_score))\n",
    "print(\"optimum coef\", (optimum_coef))\n",
    "    \n",
    "# End Your Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree\n",
    "Train and validate your <b>decision tree classifier</b>, and print out your validation(or cross-validation) error.\n",
    "> If you want, you can use cross validation, regularization, or feature selection methods. <br>\n",
    "> <b> You should use F1 score('macro' option) as evaluation metric. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "max_depth value:  11\n",
      "0.873807125795618\n",
      "0.6577983176905782\n",
      "==================================================\n",
      "max_depth value:  12\n",
      "0.9148246913137579\n",
      "0.6704339968529027\n",
      "==================================================\n",
      "max_depth value:  13\n",
      "0.9423735795394356\n",
      "0.6834243495053387\n",
      "==================================================\n",
      "max_depth value:  14\n",
      "0.9634952650614619\n",
      "0.6900164812800874\n",
      "==================================================\n",
      "max_depth value:  15\n",
      "0.977760824282085\n",
      "0.691071805555851\n",
      "==================================================\n",
      "max_depth value:  16\n",
      "0.9868866535773833\n",
      "0.6904586457086168\n",
      "==================================================\n",
      "max_depth value:  17\n",
      "0.9928450626803915\n",
      "0.6865485610194609\n",
      "==================================================\n",
      "max_depth value:  18\n",
      "0.9965230959890409\n",
      "0.693563740448218\n",
      "==================================================\n",
      "max_depth value:  19\n",
      "0.9986006066318552\n",
      "0.6951824987538001\n",
      "==================================================\n",
      "max_depth value:  20\n",
      "0.9993426785603964\n",
      "0.6889647368002778\n",
      "==================================================\n",
      "max_depth value:  21\n",
      "0.9996359645902853\n",
      "0.6916462726305399\n",
      "==================================================\n",
      "max_depth value:  22\n",
      "0.9998602130911122\n",
      "0.6909860625050441\n",
      "==================================================\n",
      "max_depth value:  23\n",
      "0.9999437181431677\n",
      "0.691605150145566\n",
      "==================================================\n",
      "max_depth value:  24\n",
      "1.0\n",
      "0.6934130884466628\n",
      "==================================================\n",
      "max_depth value:  25\n",
      "1.0\n",
      "0.6934130884466628\n",
      "==================================================\n",
      "max_depth value:  26\n",
      "1.0\n",
      "0.6934130884466628\n",
      "==================================================\n",
      "max_depth value:  27\n",
      "1.0\n",
      "0.6934130884466628\n",
      "==================================================\n",
      "max_depth value:  28\n",
      "1.0\n",
      "0.6934130884466628\n",
      "==================================================\n",
      "max_depth value:  29\n",
      "1.0\n",
      "0.6934130884466628\n",
      "==================================================\n",
      "max_depth value:  30\n",
      "1.0\n",
      "0.6934130884466628\n",
      "==================================================\n",
      "max_tr_score 0.9986006066318552\n",
      "max_va_score 0.6951824987538001\n",
      "optimum max_depth 19\n"
     ]
    }
   ],
   "source": [
    "# Training your decision tree classifier, and print out your validation(or cross-validation) error.\n",
    "# Save your own model\n",
    "# Your Code Here\n",
    "#tree = DecisionTreeClassifier(criterion='entropy', max_depth=k+1, random_state=0).fit(df_X, df_Y)\n",
    "#cross_val_score(tree, df_X, df_Y, scoring='f1_macro', cv=cv)\n",
    "\n",
    "max_tr_score = 0\n",
    "max_va_score = 0\n",
    "optimum_max_depth = -1\n",
    "\n",
    "for k in range(10, 30) :\n",
    "    print('='*50)\n",
    "    print(\"max_depth value: \", k+1)\n",
    "    model = DecisionTreeClassifier(criterion='entropy', max_depth=k+1, random_state=0)\n",
    "\n",
    "    train_score = []\n",
    "    valid_score = []\n",
    "\n",
    "    for train_index, test_index in cv.split(X) :\n",
    "        X_tr, X_va = X[train_index], X[test_index]\n",
    "        y_tr, y_va = y[train_index], y[test_index]\n",
    "\n",
    "        model.fit(X_tr, y_tr)\n",
    "        train_score.append(f1_score(y_tr, model.predict(X_tr), average='macro'))\n",
    "        valid_score.append(f1_score(y_va, model.predict(X_va), average='macro'))\n",
    "\n",
    "    print(np.mean(train_score))\n",
    "    print(np.mean(valid_score))\n",
    "    \n",
    "    if np.mean(valid_score) > max_va_score :\n",
    "        max_tr_score = np.mean(train_score)\n",
    "        max_va_score = np.mean(valid_score)\n",
    "        optimum_max_depth = k+1\n",
    "\n",
    "print('='*50)\n",
    "print(\"max_tr_score\", (max_tr_score))\n",
    "print(\"max_va_score\", (max_va_score))\n",
    "print(\"optimum max_depth\", (optimum_max_depth))\n",
    "# End Your Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "Train and validate your <b>random forest classifier</b>, and print out your validation(or cross-validation) error.\n",
    "> If you want, you can use cross validation, regularization, or feature selection methods. <br>\n",
    "> <b> You should use F1 score('macro' option) as evaluation metric. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "n_estimators value:  50\n",
      "1.0\n",
      "0.6919879629702801\n",
      "==================================================\n",
      "n_estimators value:  100\n",
      "1.0\n",
      "0.6901972843359176\n",
      "==================================================\n",
      "n_estimators value:  150\n",
      "1.0\n",
      "0.6916987375588611\n",
      "==================================================\n",
      "n_estimators value:  200\n",
      "1.0\n",
      "0.694770023439437\n",
      "==================================================\n",
      "n_estimators value:  250\n",
      "1.0\n",
      "0.6920249738834208\n",
      "==================================================\n",
      "n_estimators value:  300\n",
      "1.0\n",
      "0.6941633814345533\n",
      "==================================================\n",
      "n_estimators value:  350\n",
      "1.0\n",
      "0.6942671088868552\n",
      "==================================================\n",
      "n_estimators value:  400\n",
      "1.0\n",
      "0.6964777698193587\n",
      "==================================================\n",
      "n_estimators value:  450\n",
      "1.0\n",
      "0.6981490299128821\n",
      "==================================================\n",
      "max_tr 1.0\n",
      "max_va 0.6981490299128821\n",
      "optimum n_estimators 450\n"
     ]
    }
   ],
   "source": [
    "# parameters for GridSearchCV\n",
    "max_tr_score = 0\n",
    "max_va_score = 0\n",
    "optimum_n_estimators = -1\n",
    "\n",
    "for k in range(1, 10) :\n",
    "    print('='*50)\n",
    "    print(\"n_estimators value: \", k*50)\n",
    "    model = RandomForestClassifier(criterion='entropy', n_estimators=k*50, class_weight='balanced')\n",
    "\n",
    "    train_score = []\n",
    "    valid_score = []\n",
    "\n",
    "    for train_index, test_index in cv.split(X) :\n",
    "        X_tr, X_va = X[train_index], X[test_index]\n",
    "        y_tr, y_va = y[train_index], y[test_index]\n",
    "\n",
    "        model.fit(X_tr, y_tr)\n",
    "        train_score.append(f1_score(y_tr, model.predict(X_tr), average='macro'))\n",
    "        valid_score.append(f1_score(y_va, model.predict(X_va), average='macro'))\n",
    "\n",
    "    print(np.mean(train_score))\n",
    "    print(np.mean(valid_score))\n",
    "    \n",
    "    if np.mean(valid_score) > max_va_score :\n",
    "        max_tr_score = np.mean(train_score)\n",
    "        max_va_score = np.mean(valid_score)\n",
    "        optimum_n_estimators = k*50\n",
    "\n",
    "print('='*50)\n",
    "print(\"max_tr\", (max_tr_score))\n",
    "print(\"max_va\", (max_va_score))\n",
    "print(\"optimum n_estimators\", (optimum_n_estimators))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine\n",
    "Train and validate your <b>support vector machine classifier</b>, and print out your validation(or cross-validation) error.\n",
    "> If you want, you can use cross validation, regularization, or feature selection methods. <br>\n",
    "> <b> You should use F1 score('macro' option) as evaluation metric. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "coef value:  1.0\n",
      "0.6009615404764975\n",
      "0.5301542576348145\n",
      "==================================================\n",
      "coef value:  10.0\n",
      "0.826102262787502\n",
      "0.6760720035214024\n",
      "==================================================\n",
      "coef value:  100.0\n",
      "0.9693080586907703\n",
      "0.7297174460647307\n",
      "==================================================\n",
      "coef value:  1000.0\n",
      "0.9990514673214937\n",
      "0.7181679430918539\n",
      "==================================================\n",
      "max_tr_score 0.9693080586907703\n",
      "max_va_score 0.7297174460647307\n",
      "optimum coef 100.0\n",
      "==================================================\n",
      "==================================================\n",
      "coef value:  80.0\n",
      "0.9632512447639792\n",
      "0.7283102492162004\n",
      "==================================================\n",
      "coef value:  90.0\n",
      "0.9664980281749223\n",
      "0.7283597183270102\n",
      "==================================================\n",
      "coef value:  100.0\n",
      "0.9693080586907703\n",
      "0.7297174460647307\n",
      "==================================================\n",
      "coef value:  110.0\n",
      "0.9726880277859463\n",
      "0.7302868778537863\n",
      "==================================================\n",
      "coef value:  120.0\n",
      "0.975260247250773\n",
      "0.7282510579631164\n",
      "==================================================\n",
      "max_tr_score 0.9726880277859463\n",
      "max_va_score 0.7302868778537863\n",
      "final coef 110.0\n"
     ]
    }
   ],
   "source": [
    "# Training your support vector machine classifier, and print out your validation(or cross-validation) error.\n",
    "# Save your own model\n",
    "# Your Code Here\n",
    "\n",
    "coefs = [1.0, 10.0, 100.0, 1000.0]\n",
    "\n",
    "max_tr_score = 0\n",
    "max_va_score = 0\n",
    "optimum_coef = -1\n",
    "\n",
    "for coef in coefs:\n",
    "    print('='*50)\n",
    "    print(\"coef value: \", coef)\n",
    "    model = SVC(C=coef, class_weight='balanced')\n",
    "\n",
    "    train_score = []\n",
    "    valid_score = []\n",
    "\n",
    "    for train_index, test_index in cv.split(X) :\n",
    "        X_tr, X_va = X[train_index], X[test_index]\n",
    "        y_tr, y_va = y[train_index], y[test_index]\n",
    "    \n",
    "        model.fit(X_tr, y_tr)\n",
    "        train_score.append(f1_score(y_tr, model.predict(X_tr), average='macro'))\n",
    "        valid_score.append(f1_score(y_va, model.predict(X_va), average='macro'))\n",
    "        \n",
    "    print(np.mean(train_score))\n",
    "    print(np.mean(valid_score))\n",
    "    \n",
    "    if np.mean(valid_score) > max_va_score :\n",
    "        max_tr_score = np.mean(train_score)\n",
    "        max_va_score = np.mean(valid_score)\n",
    "        optimum_coef = coef\n",
    "\n",
    "print('='*50)\n",
    "print(\"max_tr_score\", (max_tr_score))\n",
    "print(\"max_va_score\", (max_va_score))\n",
    "print(\"optimum coef\", (optimum_coef))\n",
    "\n",
    "print('='*50)\n",
    "params = [-2, -1, 0, 1, 2]\n",
    "\n",
    "max_tr_score = 0\n",
    "max_va_score = 0\n",
    "final_coef = -1\n",
    "\n",
    "for param in params:\n",
    "    coef = optimum_coef+(optimum_coef*0.1*param)\n",
    "    print('='*50)\n",
    "    print(\"coef value: \", coef)\n",
    "    model = SVC(C=coef, class_weight='balanced')\n",
    "\n",
    "    train_score = []\n",
    "    valid_score = []\n",
    "\n",
    "    for train_index, test_index in cv.split(X) :\n",
    "        X_tr, X_va = X[train_index], X[test_index]\n",
    "        y_tr, y_va = y[train_index], y[test_index]\n",
    "    \n",
    "        model.fit(X_tr, y_tr)\n",
    "        train_score.append(f1_score(y_tr, model.predict(X_tr), average='macro'))\n",
    "        valid_score.append(f1_score(y_va, model.predict(X_va), average='macro'))\n",
    "        \n",
    "    print(np.mean(train_score))\n",
    "    print(np.mean(valid_score))\n",
    "    \n",
    "    if np.mean(valid_score) > max_va_score :\n",
    "        max_tr_score = np.mean(train_score)\n",
    "        max_va_score = np.mean(valid_score)\n",
    "        final_coef = coef\n",
    "\n",
    "print('='*50)\n",
    "print(\"max_tr_score\", (max_tr_score))\n",
    "print(\"max_va_score\", (max_va_score))\n",
    "print(\"final coef\", (final_coef))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Option) Other Classifiers.\n",
    "Train and validate other classifiers by your own manner.\n",
    "> <b> If you need, you can import other models only in this cell, only in scikit-learn. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you need additional packages, import your own packages below.\n",
    "# Your Code Here\n",
    "# End Your Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit your prediction on the test data.\n",
    "\n",
    "* Select your model and explain it briefly.\n",
    "* You should read <b>\"test.csv\"</b>.\n",
    "* Prerdict your model in array form.\n",
    "* Prediction example <br>\n",
    "[2, 6, 14, 8, $\\cdots$]\n",
    "* We will rank your result by <b>F1 metric(with 'macro' option)</b>.\n",
    "* <b> If you don't submit prediction file or submit it in wrong format, you can't get the point for this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110.0\n",
      "(2885, 23)\n",
      "(2885,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=110.0, cache_size=200, class_weight='balanced', coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Explain your final model\n",
    "\n",
    "# 18개의 class에 대한 sample 갯수가 고르지 못한 skewed data\n",
    "# data preprocessing할 때 sample이 적은 class에 대해 oversampling\n",
    "# SVM classifier의 f1 score가 가장 높아 final model로 선택\n",
    "# class_weight option을 balanced로 맞추고 error term에 대한 penalty parameter C를 overfitting되지 않는 적절한 수치로 설정\n",
    "\n",
    "print(final_coef)\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "final_model = SVC(C=final_coef, class_weight='balanced')\n",
    "final_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500, 23)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load test dataset.\n",
    "# Your Code Here\n",
    "df = pd.read_csv('./data/test.csv')\n",
    "df_X_te = df.iloc[:, 0:6].as_matrix()\n",
    "            \n",
    "# One-hot encoding\n",
    "categorical_features = [0, 2, 4]\n",
    "feature_dict = {'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5, 'g': 6, 'h': 7}\n",
    "\n",
    "for feature_num in categorical_features:\n",
    "    feature_values_te = df_X_te[:, feature_num]\n",
    "    feature_set = set(feature_values_te)\n",
    "    \n",
    "    for i, value in enumerate(feature_values_te):\n",
    "        feature_values_te[i] = feature_dict[value]\n",
    "        \n",
    "    one_hot_matrix_te = np.eye(len(feature_set))[feature_values_te.astype(int)]\n",
    "    df_X_te = np.concatenate((df_X_te, one_hot_matrix_te), axis=1)\n",
    "    \n",
    "df_X_te = np.delete(df_X_te, categorical_features, 1)\n",
    "X_te = df_X_te.astype(int)\n",
    "\n",
    "X_te.shape\n",
    "# End Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "      <th>feature4</th>\n",
       "      <th>feature5</th>\n",
       "      <th>featrure6</th>\n",
       "      <th>my_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d</td>\n",
       "      <td>1</td>\n",
       "      <td>h</td>\n",
       "      <td>5</td>\n",
       "      <td>h</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d</td>\n",
       "      <td>1</td>\n",
       "      <td>c</td>\n",
       "      <td>1</td>\n",
       "      <td>e</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c</td>\n",
       "      <td>1</td>\n",
       "      <td>d</td>\n",
       "      <td>3</td>\n",
       "      <td>e</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d</td>\n",
       "      <td>2</td>\n",
       "      <td>a</td>\n",
       "      <td>3</td>\n",
       "      <td>g</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d</td>\n",
       "      <td>1</td>\n",
       "      <td>e</td>\n",
       "      <td>1</td>\n",
       "      <td>b</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  feature1  feature2 feature3  feature4 feature5  featrure6  my_answer\n",
       "0        d         1        h         5        h          6          0\n",
       "1        d         1        c         1        e          5          3\n",
       "2        c         1        d         3        e          5          3\n",
       "3        d         2        a         3        g          8          2\n",
       "4        d         1        e         1        b          3         13"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict target class\n",
    "# Make variable \"my_answer\", type of array, and fill this array with your class predictions.\n",
    "# Modify file name into your student number and your name.\n",
    "# Your Code Here\n",
    "\n",
    "my_answer = final_model.predict(X_te)\n",
    "\n",
    "result_df = pd.read_csv('./data/test.csv')\n",
    "result_df['my_answer'] = my_answer.tolist()\n",
    "\n",
    "file_name = \"HW2_2015410056_김지윤.csv\"\n",
    "\n",
    "# End Your Code\n",
    "result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section is for saving predicted answers. DO NOT MODIFY.\n",
    "pd.Series(my_answer).to_csv(\"./data/\" + file_name, header=None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

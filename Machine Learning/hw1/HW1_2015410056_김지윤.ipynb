{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment #1 \n",
    "\n",
    "#### Machine Learning in Korea University\n",
    "#### COSE362, Fall 2018 (Prof. Jaewoo Kang)\n",
    "#### Due : 11/6 (TUE) 11:59 PM\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this assignment, you will learn model selection process among various hyperparameters.\n",
    "* Implementation detail: Anaconda 5.3 with python 3.7\n",
    "* Use given dataset. Please do not change training / validation / test split.\n",
    "* Use numpy, scikit-learn, and matplotlib library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Linear Regression with Feature Selection\n",
    "In this example we will conduct featrue selection process in linear regression model. <br>\n",
    "You will use data in 'LinReg' directory for this example. <br>\n",
    "Please perform the following steps. \n",
    "> 0. Preprocess: Change given dataset into input array for scikit-learn model.\n",
    "> 1. Feture selection : perform greedy feature selection.\n",
    "> 2. Plot: plot validation and train error against number of feature.\n",
    "> 3. Model selection and evaluation: Select best model and perform evaluation on test dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-0. Preprocess\n",
    "Load dataset and process it into appropriate array form.\n",
    "* Example <br>\n",
    "> For linear regression problem, the datasets are described onto 'dev_sample.npy', 'dev_label.npy', 'test_sample.npy', 'test_label.npy' in 'LinReg' folder. <br>\n",
    "> Load these datasets onto <b>X_dev, y_dev, X_test, y_test</b>. <br>\n",
    "> You may need to use numpy.load function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load linear regression datasets\n",
    "# Your code here\n",
    "X_dev = np.load('./LinReg/dev_sample.npy')\n",
    "y_dev = np.load('./LinReg/dev_label.npy')\n",
    "X_test = np.load('./LinReg/test_sample.npy')\n",
    "y_test = np.load('./LinReg/test_label.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-1. Feature selection\n",
    "Build linear regression models with different number of features. (1 ~ 100)<br>\n",
    "Please use <b>cross validation</b>, <b>greedy approach</b> for feature selection until choose optimal number of features. <br> \n",
    "\n",
    "* For cross validaton, you need to split your development set into 5-fold. This is implemented into class <b>cv</b>.\n",
    "* Feature selection example : Input with 10 features\n",
    "> Call 10 features as #1, #2, #3, ..., #10 <br>\n",
    "> First build 10 models with only one feature. \n",
    "> Compare model with #1, model with #2, ... , model with #10 <br>\n",
    "> Choose feature of the best model. (for example, #1 is the best) <br>\n",
    "> Build model with 2 features. (#1, #2), (#1, #3), ..., (#1, #10). <br>\n",
    "> Then, add feature with the best performance. <br>\n",
    "> And so on...\n",
    "\n",
    "<b>For the next step, please save validation and train error of the best model for each number of selected features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "# of selected feature(s) : 1\n",
      "Selected feature of this iteration : 1\n",
      "==================================================\n",
      "# of selected feature(s) : 2\n",
      "Selected feature of this iteration : 3\n",
      "==================================================\n",
      "# of selected feature(s) : 3\n",
      "Selected feature of this iteration : 45\n",
      "==================================================\n",
      "# of selected feature(s) : 4\n",
      "Selected feature of this iteration : 109\n",
      "==================================================\n",
      "# of selected feature(s) : 5\n",
      "Selected feature of this iteration : 53\n",
      "==================================================\n",
      "# of selected feature(s) : 6\n",
      "Selected feature of this iteration : 43\n",
      "==================================================\n",
      "# of selected feature(s) : 7\n",
      "Selected feature of this iteration : 21\n",
      "==================================================\n",
      "# of selected feature(s) : 8\n",
      "Selected feature of this iteration : 95\n",
      "==================================================\n",
      "# of selected feature(s) : 9\n",
      "Selected feature of this iteration : 116\n",
      "==================================================\n",
      "# of selected feature(s) : 10\n",
      "Selected feature of this iteration : 79\n",
      "==================================================\n",
      "# of selected feature(s) : 11\n",
      "Selected feature of this iteration : 15\n",
      "==================================================\n",
      "# of selected feature(s) : 12\n",
      "Selected feature of this iteration : 22\n",
      "==================================================\n",
      "# of selected feature(s) : 13\n",
      "Selected feature of this iteration : 30\n",
      "==================================================\n",
      "# of selected feature(s) : 14\n",
      "Selected feature of this iteration : 46\n",
      "==================================================\n",
      "# of selected feature(s) : 15\n",
      "Selected feature of this iteration : 13\n",
      "==================================================\n",
      "# of selected feature(s) : 16\n",
      "Selected feature of this iteration : 63\n",
      "==================================================\n",
      "# of selected feature(s) : 17\n",
      "Selected feature of this iteration : 93\n",
      "==================================================\n",
      "# of selected feature(s) : 18\n",
      "Selected feature of this iteration : 54\n",
      "==================================================\n",
      "# of selected feature(s) : 19\n",
      "Selected feature of this iteration : 92\n",
      "==================================================\n",
      "# of selected feature(s) : 20\n",
      "Selected feature of this iteration : 89\n",
      "==================================================\n",
      "# of selected feature(s) : 21\n",
      "Selected feature of this iteration : 2\n",
      "==================================================\n",
      "# of selected feature(s) : 22\n",
      "Selected feature of this iteration : 12\n",
      "==================================================\n",
      "# of selected feature(s) : 23\n",
      "Selected feature of this iteration : 117\n",
      "==================================================\n",
      "# of selected feature(s) : 24\n",
      "Selected feature of this iteration : 96\n",
      "==================================================\n",
      "# of selected feature(s) : 25\n",
      "Selected feature of this iteration : 115\n",
      "==================================================\n",
      "# of selected feature(s) : 26\n",
      "Selected feature of this iteration : 37\n",
      "==================================================\n",
      "# of selected feature(s) : 27\n",
      "Selected feature of this iteration : 91\n",
      "==================================================\n",
      "# of selected feature(s) : 28\n",
      "Selected feature of this iteration : 35\n",
      "==================================================\n",
      "# of selected feature(s) : 29\n",
      "Selected feature of this iteration : 62\n",
      "==================================================\n",
      "# of selected feature(s) : 30\n",
      "Selected feature of this iteration : 36\n",
      "==================================================\n",
      "# of selected feature(s) : 31\n",
      "Selected feature of this iteration : 99\n",
      "==================================================\n",
      "# of selected feature(s) : 32\n",
      "Selected feature of this iteration : 19\n",
      "==================================================\n",
      "# of selected feature(s) : 33\n",
      "Selected feature of this iteration : 124\n",
      "==================================================\n",
      "# of selected feature(s) : 34\n",
      "Selected feature of this iteration : 24\n",
      "==================================================\n",
      "# of selected feature(s) : 35\n",
      "Selected feature of this iteration : 6\n",
      "==================================================\n",
      "# of selected feature(s) : 36\n",
      "Selected feature of this iteration : 9\n",
      "==================================================\n",
      "# of selected feature(s) : 37\n",
      "Selected feature of this iteration : 34\n",
      "==================================================\n",
      "# of selected feature(s) : 38\n",
      "Selected feature of this iteration : 66\n",
      "==================================================\n",
      "# of selected feature(s) : 39\n",
      "Selected feature of this iteration : 16\n",
      "==================================================\n",
      "# of selected feature(s) : 40\n",
      "Selected feature of this iteration : 122\n",
      "==================================================\n",
      "# of selected feature(s) : 41\n",
      "Selected feature of this iteration : 76\n",
      "==================================================\n",
      "# of selected feature(s) : 42\n",
      "Selected feature of this iteration : 75\n",
      "==================================================\n",
      "# of selected feature(s) : 43\n",
      "Selected feature of this iteration : 61\n",
      "==================================================\n",
      "# of selected feature(s) : 44\n",
      "Selected feature of this iteration : 14\n",
      "==================================================\n",
      "# of selected feature(s) : 45\n",
      "Selected feature of this iteration : 94\n",
      "==================================================\n",
      "# of selected feature(s) : 46\n",
      "Selected feature of this iteration : 125\n",
      "==================================================\n",
      "# of selected feature(s) : 47\n",
      "Selected feature of this iteration : 100\n",
      "==================================================\n",
      "# of selected feature(s) : 48\n",
      "Selected feature of this iteration : 98\n",
      "==================================================\n",
      "# of selected feature(s) : 49\n",
      "Selected feature of this iteration : 71\n",
      "==================================================\n",
      "# of selected feature(s) : 50\n",
      "Selected feature of this iteration : 111\n",
      "==================================================\n",
      "# of selected feature(s) : 51\n",
      "Selected feature of this iteration : 114\n",
      "==================================================\n",
      "# of selected feature(s) : 52\n",
      "Selected feature of this iteration : 47\n",
      "==================================================\n",
      "# of selected feature(s) : 53\n",
      "Selected feature of this iteration : 107\n",
      "==================================================\n",
      "# of selected feature(s) : 54\n",
      "Selected feature of this iteration : 49\n",
      "==================================================\n",
      "# of selected feature(s) : 55\n",
      "Selected feature of this iteration : 38\n",
      "==================================================\n",
      "# of selected feature(s) : 56\n",
      "Selected feature of this iteration : 39\n",
      "==================================================\n",
      "# of selected feature(s) : 57\n",
      "Selected feature of this iteration : 113\n",
      "==================================================\n",
      "# of selected feature(s) : 58\n",
      "Selected feature of this iteration : 0\n",
      "==================================================\n",
      "# of selected feature(s) : 59\n",
      "Selected feature of this iteration : 108\n",
      "==================================================\n",
      "# of selected feature(s) : 60\n",
      "Selected feature of this iteration : 85\n",
      "==================================================\n",
      "# of selected feature(s) : 61\n",
      "Selected feature of this iteration : 32\n",
      "==================================================\n",
      "# of selected feature(s) : 62\n",
      "Selected feature of this iteration : 11\n"
     ]
    }
   ],
   "source": [
    "# Define linear regression function\n",
    "# You may use sklearn.linear_model.LinearRegression\n",
    "# Your code here\n",
    "lr=LinearRegression()\n",
    "# End your code\n",
    "\n",
    "# Basic settings. DO NOT MODIFY\n",
    "selected_feature = []\n",
    "sel_num = 100\n",
    "valid_split = 1/5\n",
    "cv = ShuffleSplit(n_splits=5, test_size=valid_split, random_state=0)\n",
    "\n",
    "selected_train_error = []\n",
    "selected_valid_error = []\n",
    "\n",
    "# For greedy selection\n",
    "for sel in range(sel_num) :\n",
    "    min_train_error = +1000\n",
    "    min_valid_error = +1000\n",
    "    min_feature = 0\n",
    "    \n",
    "    # For each feature\n",
    "    for i in range(X_dev.shape[1]) : #i=0~125\n",
    "        train_error_ith = []\n",
    "        valid_error_ith = []\n",
    "        # Select feature greedy\n",
    "        # Hint : There should be no duplicated feature in selected_feature\n",
    "        # Your code here\n",
    "        \n",
    "        X_dev_fs = [] # define empty array\n",
    "        X_temp = []\n",
    "        f_list = selected_feature.copy() # features those are already selected\n",
    "\n",
    "        if i not in f_list: #pass over if ith one is duplicated feature\n",
    "            pass\n",
    "        else:\n",
    "            continue\n",
    "        f_list.append(i) # features for testing\n",
    "        for k in range(len(f_list)):\n",
    "            f_col = X_dev[:, f_list[k]]\n",
    "            X_temp.append(f_col)\n",
    "        \n",
    "        X_temp = np.asarray(X_temp)\n",
    "        for m in range((X_temp.shape[1])):\n",
    "            sample_list = []\n",
    "            for n in range(X_temp.shape[0]):\n",
    "                sample_list.append(X_temp[n][m])\n",
    "            X_dev_fs.append(sample_list)\n",
    "        \n",
    "        X_dev_fs = np.asarray(X_dev_fs)\n",
    "        # End your code\n",
    "        \n",
    "        # For cross validation\n",
    "        for train_index, test_index in cv.split(X_dev) :\n",
    "            X_train, X_valid = X_dev_fs[train_index], X_dev_fs[test_index]\n",
    "            y_train, y_valid = y_dev[train_index], y_dev[test_index]\n",
    "\n",
    "            # Derive training error, validation error\n",
    "            # You may use sklearn.metrics.mean_squared_error, model.fit(), model.predict()\n",
    "            # Your code here\n",
    "            lr_model = lr.fit(X_train, y_train)\n",
    "            pred_train = lr_model.predict(X_train)\n",
    "            pred_valid = lr_model.predict(X_valid)\n",
    "            \n",
    "            train_error_ith.append(mean_squared_error(pred_train, y_train))\n",
    "            valid_error_ith.append(mean_squared_error(pred_valid, y_valid))\n",
    "            # End your code\n",
    "            \n",
    "        # Select best performance feature set on each features\n",
    "        # You should choose the feature which has minimum mean cross validation error\n",
    "        # Your code here\n",
    "        if min_train_error > np.min(train_error_ith):\n",
    "            min_train_error = np.min(train_error_ith)\n",
    "        if min_valid_error > np.min(valid_error_ith):\n",
    "            min_valid_error = np.min(valid_error_ith)\n",
    "            min_feature = i\n",
    "        # End your code\n",
    "\n",
    "    print('='*50)\n",
    "    print(\"# of selected feature(s) : {}\".format(sel+1))\n",
    "    print(\"Selected feature of this iteration : {}\".format(min_feature))\n",
    "    selected_feature.append(min_feature)\n",
    "    selected_train_error.append(min_train_error)\n",
    "    selected_valid_error.append(min_valid_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2. Plot error\n",
    "Plot train and validation error against number of features.<br>\n",
    "After plotting, <b>analyze the result graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x27493ddd048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train error plot\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(np.arange(1,sel_num+1), selected_train_error)\n",
    "plt.title('Training error')\n",
    "plt.legend(['error'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x27493e17e80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Validation error plot\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(np.arange(1,sel_num+1), selected_valid_error)\n",
    "plt.title('Validation error')\n",
    "plt.legend(['error'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze\n",
    "Write explanation of graph below. <br>\n",
    "Analyze the folloing points.\n",
    "* Trend of each error against number of features\n",
    "* Meaning of gap between vlidation error and train error\n",
    "* Meaning of each region in graph\n",
    "* Others..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write description here\n",
    "training error는 사용하는 feature가 많아질수록 감소하고 <br>\n",
    "validation error는 일정 수준 감소하다가 다시 증가한다. <br><br>\n",
    "\n",
    "training error와 validation error의 두 그래프를 비교해보면 overfitting 인지 underfitting인지 알 수 있는데, <br>\n",
    "training error와 validation error의 갭이 작고 두개 모두 값이 크면 underfitting, <br>\n",
    "training error의 값은 작은데 validation error의 값은 크게 나타나서 갭이 크면 overfitting이다. <br>\n",
    "training error와 validation error의 값이 비슷하게 작은 상태가 가장 이상적인 상태이다. (validation error가 minimum일 때) <br><br>\n",
    "\n",
    "validation error가 minimum인 점을 기준으로<br>\n",
    "왼편은 high bias, 오른편은 high variance이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-3. Model selection and evaluation\n",
    "Select the best model and perform a test on test dataset.<br>\n",
    "Print the <b>performance on test set</b> with <b>features of the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "# of selected features : 30\n",
      "Selected features : \n",
      "[1, 3, 45, 109, 53, 44, 43, 116, 95, 21, 77, 80, 97, 26, 24, 36, 40, 117, 92, 84, 110, 98, 15, 6, 118, 85, 113, 0, 69, 99]\n",
      "Training error : 5.707160062659024\n",
      "Validation error : 4.522828368608584\n",
      "Test error : 7.679010560715302\n"
     ]
    }
   ],
   "source": [
    "# Select optimal feature set corresponding the minimum cross validation error\n",
    "# Your code here\n",
    "optimal_idx = np.argmin(selected_valid_error)\n",
    "selected_feature = selected_feature[0:optimal_idx+1]\n",
    "\n",
    "X_dev_fs = []\n",
    "X_temp = []\n",
    "    \n",
    "for k in range(len(selected_feature)):\n",
    "    f_col = X_dev[:, selected_feature[k]]\n",
    "    X_temp.append(f_col)\n",
    "        \n",
    "X_temp = np.asarray(X_temp)\n",
    "for m in range((X_temp.shape[1])):\n",
    "    sample_list = []\n",
    "    for n in range(X_temp.shape[0]):\n",
    "        sample_list.append(X_temp[n][m])\n",
    "    X_dev_fs.append(sample_list)\n",
    "        \n",
    "X_dev_fs = np.asarray(X_dev_fs)\n",
    "# End your code\n",
    "\n",
    "# Basic settings. DO NOT MODIFY\n",
    "min_train_error = 1000\n",
    "min_valid_error = 1000\n",
    "optimal_param = np.array([])\n",
    "\n",
    "for train_index, test_index in cv.split(X_dev) :\n",
    "    X_train, X_valid = X_dev_fs[train_index], X_dev_fs[test_index]\n",
    "    y_train, y_valid = y_dev[train_index], y_dev[test_index]\n",
    "    \n",
    "    # Derive training error, validation error for each fold\n",
    "    # For each fold, you need to compare error with previous minimum error.\n",
    "    # Your code here\n",
    "    lr_model = lr.fit(X_train, y_train)\n",
    "    pred_train = lr_model.predict(X_train)\n",
    "    pred_valid = lr_model.predict(X_valid)\n",
    "    \n",
    "    train_error = mean_squared_error(pred_train, y_train)\n",
    "    valid_error = mean_squared_error(pred_valid, y_valid)\n",
    "    \n",
    "    if min_train_error > train_error:\n",
    "        min_train_error = train_error\n",
    "    if min_valid_error > valid_error:\n",
    "        min_valid_error = valid_error\n",
    "        optimal_model = lr.fit(X_train, y_train)\n",
    "    # End your code\n",
    "\n",
    "# Find the best model on each fold\n",
    "# Derive test error with best performance model\n",
    "# Your code here\n",
    "X_test_fs = []\n",
    "X_test_temp = []\n",
    "    \n",
    "for k in range(len(selected_feature)):\n",
    "    f_col = X_test[:, selected_feature[k]]\n",
    "    X_test_temp.append(f_col)\n",
    "        \n",
    "X_test_temp = np.asarray(X_test_temp)\n",
    "for m in range((X_test_temp.shape[1])):\n",
    "    sample_list = []\n",
    "    for n in range(X_test_temp.shape[0]):\n",
    "        sample_list.append(X_test_temp[n][m])\n",
    "    X_test_fs.append(sample_list)\n",
    "        \n",
    "X_test_fs = np.asarray(X_test_fs)\n",
    "\n",
    "pred_test = optimal_model.predict(X_test_fs)\n",
    "test_error = mean_squared_error(pred_test, y_test)\n",
    "# End your code\n",
    "\n",
    "# Drop features of final model\n",
    "print(\"Results\")\n",
    "print(\"# of selected features : {}\".format(len(selected_feature)))\n",
    "print(\"Selected features : \")\n",
    "print(selected_feature)\n",
    "\n",
    "# Drop test error and accuracy\n",
    "print(\"Training error : {}\".format(min_train_error))\n",
    "print(\"Validation error : {}\".format(min_valid_error))\n",
    "print(\"Test error : {}\".format(test_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Logistic Regression with Regularizer\n",
    "\n",
    "In this example you will explore the effect of regularization parameter.<br>\n",
    "You will use <b>'Heart Disease Dataset'</b> in <b>'LogReg'</b> for this example. <br>\n",
    "\n",
    "The goal is to predict the presence of heart disease given attributes of a patient.<br>\n",
    "The presence is integer valued from 0 (no presence) to 4, but you have to only distingush presensence (values 1,2,3,4) from absence (value 0). <br>\n",
    "Each attribute is described below. <br>\n",
    "\n",
    "> 1. age : age in years <br>\n",
    "> 2. sex : sex (1 = male; 0 = female) <br>\n",
    "> 3. cp : chest pain type <br>\n",
    "-- Value 1: typical angina <br>\n",
    "-- Value 2: atypical angina <br>\n",
    "-- Value 3: non-anginal pain <br>\n",
    "-- Value 4: asymptomatic  <br>\n",
    "> 4. trestbps : resting blood pressure (in mm Hg on admission to the hospital)  <br>\n",
    "> 5. chol : serum cholestoral in mg/dl  <br>\n",
    "> 6. fbs : (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false) <br>\n",
    "> 7. restecg  : resting electrocardiographic results <br>\n",
    "-- Value 0: normal <br>\n",
    "-- Value 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV) <br>\n",
    "-- Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria <br>\n",
    "> 8. thalach : maximum heart rate achieved <br>\n",
    "> 9. exang : exercise induced angina (1 = yes; 0 = no) <br>\n",
    "> 10. oldpeak : ST depression induced by exercise relative to rest <br>\n",
    "> 11. slope : the slope of the peak exercise ST segment <br>\n",
    "-- Value 1: upsloping <br>\n",
    "-- Value 2: flat <br>\n",
    "-- Value 3: downsloping  <br>\n",
    "> 12. ca : number of major vessels (0-3) colored by flourosopy  <br>\n",
    "> 13. thal : 3 = normal; 6 = fixed defect; 7 = reversable defect  <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-0. Preprocess\n",
    "\n",
    "Firstly, read training, validation and test datasets respectively. <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_data(data_type):\n",
    "    f = open('./LogReg/' + data_type + '.data', 'r')\n",
    "\n",
    "    X, Y = [],[]\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        if not line: break\n",
    "        spl = line.split(',')\n",
    "        x = spl[:-1]\n",
    "        y = int(spl[-1])\n",
    "        \n",
    "        X.append(list(map(float, x)))\n",
    "        \n",
    "        # Define the variable 'binary_label'                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  .\n",
    "        # Note that labels must be 1 or 0.\n",
    "        # Your code here\n",
    "        if y == 0:\n",
    "            binary_label = 0\n",
    "        else:\n",
    "            binary_label = 1\n",
    "        \n",
    "        Y.append(binary_label)  # blank\n",
    "    \n",
    "    return X, Y\n",
    "\n",
    "X_tr, Y_tr = read_data('train')\n",
    "X_va, Y_va = read_data('valid')\n",
    "X_te, Y_te = read_data('test')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization and Converting to one-hot vector\n",
    "\n",
    "Data preprocessing takes several steps after loading data. <br>\n",
    "1. <b>Normailze</b> numerical values. Normalization is defined as <b><i>normalized_value</i> = (value - mean) / std</b>. <br>\n",
    "   You should calculate mean and standard deviation (std) on <b> train data </b> and normalize train, valid and test data.\n",
    "2. For categorical attributes, <b>build dictionaries</b> of each attribute and convert the categorical values to <b>one-hot vectors</b>. <br>\n",
    "3. Concatenate all the obtained values. <br>\n",
    "\n",
    "If you have done correctly, you will get results that are same format as below: \n",
    "* <b>before</b> : [63.0, 1.0, 1.0, 145.0, 233.0, 1.0, 2.0, 150.0, 0.0, 2.3, 3.0, 0.0, 6.0]\n",
    "* <b>after</b> : [0.11099784710934087, 0, 1, 1, 0, 0, 0, 0.035386000081823056, -0.005256085700922788, 0, 1, 0, 0, 1, 0.0026598418293161848, 1, 0, 0.6659671864819814, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0] <br>\n",
    "(The values in the above example can be different from actual values.)<br>\n",
    "\n",
    "<b>Do not use any library such as sklearn.preprocessing. You can use only Numpy. </b><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.831249403246673, 1, 0, 0, 0, 0, 1, -0.4577077444521834, -0.7265827532507144, 1, 0, 1, 0, 0, 0.6147522746311291, 1, 0, -0.8976077290840021, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n",
      "[0.24600221003004147, 1, 0, 0, 0, 0, 1, 0.396197612786674, -0.14034846344842877, 1, 0, 1, 0, 0, -1.084094760649275, 0, 1, -0.7441037986029699, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1]\n",
      "[ 0.8312494   1.          0.          0.          0.          0.\n",
      "  1.         -0.45770774 -0.72658275  1.          0.          1.\n",
      "  0.          0.          0.61475227  1.          0.         -0.89760773\n",
      "  1.          0.          0.          1.          0.          0.\n",
      "  0.          1.          0.          0.        ]\n",
      "[ 0.24600221  1.          0.          0.          0.          0.\n",
      "  1.          0.39619761 -0.14034846  1.          0.          1.\n",
      "  0.          0.         -1.08409476  0.          1.         -0.7441038\n",
      "  0.          1.          0.          1.          0.          0.\n",
      "  0.          0.          0.          1.        ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.94829884,  0.        ,  1.        , ...,  0.        ,\n",
       "         1.        ,  0.        ],\n",
       "       [ 1.4164966 ,  0.        ,  1.        , ...,  1.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 1.4164966 ,  0.        ,  1.        , ...,  0.        ,\n",
       "         0.        ,  1.        ],\n",
       "       ...,\n",
       "       [ 0.36305165,  0.        ,  1.        , ...,  0.        ,\n",
       "         1.        ,  0.        ],\n",
       "       [-1.62678881,  0.        ,  1.        , ...,  1.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.24600221,  1.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  1.        ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Your code here\n",
    "mean_list = []\n",
    "std_list = []\n",
    "\n",
    "num_attr = [0,3,4,7,9]\n",
    "cat_attr = [1,2,5,6,8,10,11,12]\n",
    "\n",
    "def convert(X_list):\n",
    "    # make mean & std list of numerical attributes\n",
    "    for i in range(len(X_list[0])): # number of attribute = 13\n",
    "        if i not in num_attr:\n",
    "            mean_list.append(-1)\n",
    "            std_list.append(-1)\n",
    "            continue\n",
    "        else:\n",
    "            pass\n",
    "        attr_list = []\n",
    "        for j in range(len(X_list)): # number of sample = 118\n",
    "            attr_list.append(X_list[j][i])\n",
    "        mean_list.append(np.mean(attr_list))\n",
    "        std_list.append(np.std(attr_list))\n",
    "    \n",
    "    # make dictionary of categorical attributes\n",
    "    dic2 = {1.0: [0,1], 0.0: [1,0]}\n",
    "    dic3 = {1.0: [1,0,0,0], 2.0: [0,1,0,0], 3.0: [0,0,1,0], 4.0: [0,0,0,1]}\n",
    "    dic6 = {1.0: [0,1], 0.0: [1,0]}\n",
    "    dic7 = {0.0: [1,0,0], 1.0: [0,1,0], 2.0: [0,0,1]}\n",
    "    dic9 = {1.0: [0,1], 0.0: [1,0]}\n",
    "    dic11 = {1.0: [1,0,0], 2.0: [0,1,0], 3.0: [0,0,1]}\n",
    "    dic12 = {0.0: [1,0,0,0], 1.0: [0,1,0,0], 2.0: [0,0,1,0], 3.0: [0,0,0,1]}\n",
    "    dic13 = {3.0: [1,0,0], 6.0: [0,1,0], 7.0: [0,0,1]}\n",
    "\n",
    "    cat_dic = {1:dic2, 2:dic3, 5:dic6, 6:dic7, 8:dic9, 10:dic11, 11:dic12, 12:dic13}\n",
    "\n",
    "    converted_X_list = []\n",
    "    for i in range(len(X_list)):\n",
    "        converted_row = []\n",
    "        for j in range(len(X_list[0])):\n",
    "            if j in num_attr:\n",
    "                converted_row.append((X_list[i][j] - mean_list[j]) / std_list[j])\n",
    "            else:\n",
    "                converted_row = converted_row + cat_dic[j][X_list[i][j]]\n",
    "        converted_X_list.append(converted_row)\n",
    "    \n",
    "    return converted_X_list\n",
    "\n",
    "converted_tr = convert(X_tr)\n",
    "converted_va = convert(X_va)\n",
    "converted_te = convert(X_te)\n",
    "\n",
    "print(converted_tr[53])\n",
    "print(converted_tr[117])\n",
    "\n",
    "converted_tr = np.asarray(converted_tr)\n",
    "print(converted_tr[53])\n",
    "print(converted_tr[117])\n",
    "type(converted_tr)\n",
    "converted_tr\n",
    "# End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1. Logistic regression model and regularizer\n",
    "Build logistic regression model with l2 regularization utilizing sklearn. <br>\n",
    "Find the optimal coefficient based on <b>cross entropy loss</b> on thet validation set. <br>\n",
    "Try following regularization parameter settings.\n",
    "* Regularization parameters = 0.01, 0.05, 0.1, 0.5, 1, 10, 100 <br>\n",
    "* Note that regluarization parameter for LogisticRegression in sklearn is inverse of true parameter. <br>\n",
    "  (coef = 0.001 for LogisticRegression   =>  $\\lambda$ = 1000 in our course note)\n",
    "* Your model should be <b>LogisticRegression(C=coef, solver='lbfgs', max_iter=500). </b>\n",
    "  <br>  <b>Do not change the model setting except C. </b> \n",
    "  <br> (coef = each regularization parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7.373554456993993,\n",
       " 5.821240915678567,\n",
       " 5.433155792169119,\n",
       " 5.433164776409907,\n",
       " 5.045079652900459,\n",
       " 5.433173760650696,\n",
       " 5.433173760650696]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Please use below function\n",
    "# logreg = LogisticRegression(C=coef, solver='lbfgs', max_iter=500)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "coefs = [0.01, 0.05, 0.1, 0.5, 1, 10, 100]\n",
    "\n",
    "opt_coef = 1\n",
    "\n",
    "\n",
    "# To plot losses on training and validation sets with varied parameter settings, \n",
    "# save them on lists.\n",
    "loss_tr, loss_va = [],[]\n",
    "\n",
    "# Your code here\n",
    "\n",
    "for i in range(len(coefs)):\n",
    "    clf = LogisticRegression(C=coefs[i], solver='lbfgs', max_iter=500).fit(converted_tr, Y_tr)\n",
    "    \n",
    "    pred_tr = clf.predict(converted_tr)\n",
    "    pred_va = clf.predict(converted_va)\n",
    "    \n",
    "    loss_tr.append(log_loss(pred_tr, Y_tr))\n",
    "    loss_va.append(log_loss(pred_va, Y_va))\n",
    "\n",
    "# End your code\n",
    "\n",
    "loss_va\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2. Plot error\n",
    "Plot the train and validation loss against given regularization parameter <b>(not inverse)</b>.<br>\n",
    "<b> Analyze the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13d6bdfff98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Do not fix the code!!\n",
    "\n",
    "plt.plot(coefs, loss_tr, coefs, loss_va, 'r-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze \n",
    "Write explanation of graph below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ coef값은 실제 regularization parameter λ의 inverse값이므로 0.01, 0.05, 0.1, 0.5, 1, 10, 100은 각각 λ값 100, 20, 10, 2, 1, 0,1 0,01을 의미한다.\n",
    "+ coef값이 커질수록 λ값이 작아질수록 regularization을 약하게 하는 것을 의미한다.\n",
    "+ 가장 큰 λ값을 적용한 제일 왼쪽이 regularization을 강하게 적용한 모델의 loss를 나타낸다.\n",
    "<br><br>\n",
    "\n",
    "+ regularization을 강하게 적용할수록 (λ값이 클수록) attribute의 영향력이 낮아져 단순한 모델이 나온다.\n",
    "+ 반대로 λ값이 작다면 attribute value의 영향력이 높은, 학습 데이터에 더 강하게 fitting된 복잡한 모델이 나온다.\n",
    "<br><br>\n",
    "\n",
    "그래프를 오른쪽에서 왼쪽으로 λ값이 작은 경우부터 큰 경우까지 차례대로 본다면\n",
    "+ training loss는 점점 커진다.\n",
    "+ validation loss는 점점 줄어들다가 다시 증가한다.\n",
    "+ validation loss의 최저점을 기준으로 오른편으로 갈수록 일반화가 덜 된 모델이라고 볼 수 있고 overfitting(high variance)상태이기 때문에 training loss는 작은 반면 validation loss는 크다.\n",
    "+ 왼편으로 갈수록 단순화된 모델이라고 볼 수 있고, underfitting(high bias)상태이기 때문에 training loss와 validation loss가 모두 크게 나타난다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-3. Model selection and evaluation\n",
    "\n",
    "Drop the performance on test set with the regularization coefficient of the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal : 1, Loss : 3.220, Accuracy : 83.33\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Your code here\n",
    "optimal_idx = np.argmin(loss_va)\n",
    "coef = coefs[optimal_idx]\n",
    "\n",
    "clf = LogisticRegression(C=coef, solver='lbfgs', max_iter=500).fit(converted_tr, Y_tr)\n",
    "pred_te = clf.predict(converted_te)\n",
    "test_loss = log_loss(pred_tr, Y_tr)\n",
    "test_acc = clf.score(converted_te, Y_te)\n",
    "# End your code\n",
    "\n",
    "\n",
    "#print regularization paramter of final model and drop test loss and accuracy\n",
    "print (\"Optimal : {}, Loss : {:2.3f}, Accuracy : {:3.2f}\".format(coef, test_loss, test_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
